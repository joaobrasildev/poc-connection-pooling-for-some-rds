# DECISIONS.md ‚Äî Registro de Decis√µes T√©cnicas

> **Prop√≥sito:** evitar que o agente de IA sugira refatorar algo que j√° foi decidido
> por um bom motivo, e preservar o "porqu√™" por tr√°s de escolhas n√£o-√≥bvias.
>
> **Formato:** ADR leve (Architecture Decision Record). Cada decis√£o tem contexto,
> alternativas consideradas, escolha feita e consequ√™ncias.
>
> **√öltima atualiza√ß√£o:** 2026-02-18 (Fase 3 conclu√≠da)

---

## ADR-001: TCP Relay Transparente (io.Copy) em vez de TDS Parsing p√≥s Pre-Login

**Fase:** 2 ‚Äî TDS Wire Protocol Proxy \
**Status:** Aceita (em vigor) \
**Data:** 2026-02 (sess√£o de implementa√ß√£o da Fase 2)

### Contexto
O proxy precisa intermediar conex√µes TDS entre client e SQL Server. A abordagem
inicial era fazer parsing completo de todos os pacotes TDS (Pre-Login ‚Üí Login7 ‚Üí
Data Phase) com o proxy terminando TLS e re-encriptando.

### Problema
O SQL Server 2022 exige TLS por padr√£o (`ENCRYPT_ON`). Quando o proxy tentava
interceptar o Pre-Login e responder `ENCRYPT_NOT_SUP`, o driver do client e o
backend discordavam sobre o n√≠vel de encripta√ß√£o, causando falha de TLS handshake.
Tentar fazer TLS termination no proxy introduzia complexidade enorme (certificados,
re-encrypt, parsing de TDS dentro de TLS records).

### Decis√£o
Ap√≥s o Pre-Login, o proxy faz **relay TCP transparente** via `io.Copy` bidirecional.
O Pre-Login √© o √∫nico pacote TDS que o proxy l√™ e faz forward. Tudo depois
(TLS handshake, Login7, queries, respostas) passa como bytes opacos.

### Alternativas descartadas
1. **TLS termination no proxy** ‚Äî complexidade desproporcional para um POC
2. **For√ßar ENCRYPT_NOT_SUP** ‚Äî SQL Server 2022 rejeita, clients modernos tamb√©m
3. **Parsing TDS dentro de TLS records** ‚Äî imposs√≠vel sem terminar TLS primeiro

### Consequ√™ncias
- ‚úÖ Funciona com qualquer configura√ß√£o de TLS do SQL Server
- ‚úÖ Zero overhead de parsing durante transfer√™ncia de dados
- ‚ùå O proxy **n√£o consegue inspecionar pacotes** durante a fase de dados (TLS opaco)
- ‚ùå **Pinning detection n√£o funciona** com `io.Copy` ‚Äî precisa de solu√ß√£o diferente na Fase 4
- ‚ùå **Routing por Login7 n√£o funciona** ‚Äî o Login7 est√° dentro do stream TLS
- ‚ùå M√©tricas TDS (`TDSPacketsTotal`, `QueryDuration`) n√£o s√£o populadas

### Impacto nas pr√≥ximas fases
Para a Fase 4 (pinning), as op√ß√µes s√£o:
1. Implementar TLS termination no proxy (complexo, mas habilita tudo)
2. Usar `tds.Relay` com callback apenas em modo `ENCRYPT_NOT_SUP` (limitado)
3. Implementar pinning inferido (pin por sess√£o inteira, sem detec√ß√£o granular)
4. Usar informa√ß√µes out-of-band (query no SQL Server para detectar transa√ß√µes abertas)

---

## ADR-002: Lua Scripts via EVALSHA em vez de Redis Transactions

**Fase:** 3 ‚Äî Coordena√ß√£o Distribu√≠da \
**Status:** Aceita (em vigor)

### Contexto
O acquire de conex√£o precisa ser at√¥mico: verificar `count < max` e incrementar
em uma √∫nica opera√ß√£o. Duas abordagens principais no Redis:
MULTI/EXEC (transactions) ou Lua scripts (EVAL/EVALSHA).

### Decis√£o
Usar **Lua scripts embeddados** (`//go:embed`) executados via `EVALSHA`.

### Raz√µes
1. **Atomicidade real** ‚Äî Lua executa atomicamente no Redis, sem window entre GET e INCR
2. **Menos round-trips** ‚Äî uma chamada faz GET+compare+INCR+HINCRBY, vs 4 comandos separados
3. **Scripts cachados** ‚Äî `ScriptLoad` no startup, depois `EvalSha` (apenas o hash trafega)
4. **L√≥gica condicional** ‚Äî o script retorna -1 (lotado) ou -2 (n√£o configurado) sem extra RTT
5. **Efeitos colaterais** ‚Äî release.lua faz PUBLISH no mesmo script (notifica√ß√£o + decrement at√¥micos)

### Alternativas descartadas
- **MULTI/EXEC** ‚Äî n√£o suporta l√≥gica condicional (GET dentro de MULTI sempre retorna QUEUED)
- **WATCH/MULTI** ‚Äî funciona mas com retry loop, mais complexo e mais lento sob contention
- **Redlock** ‚Äî overkill para contagem at√¥mica, apropriado para locks exclusivos

### Consequ√™ncias
- ‚úÖ Opera√ß√£o verdadeiramente at√¥mica, sem race conditions
- ‚úÖ Performance: 1 RTT por acquire/release
- ‚ùå Lua scripts n√£o funcionam em Redis Cluster com keys em slots diferentes
  (aceit√°vel: POC usa Redis standalone)

---

## ADR-003: Pool *sql.DB N√ÉO √© Usado para Tr√°fego TDS

**Fase:** 1 + 2 \
**Status:** Aceita (em vigor)

### Contexto
A Fase 1 criou um pool de `*sql.DB` connections (via go-mssqldb) para cada bucket.
A Fase 2 implementou o proxy TDS usando `net.Conn` diretas (TCP relay).

### Decis√£o
O pool `*sql.DB` **continua existindo** mas N√ÉO √© usado para o tr√°fego de dados TDS.
As sess√µes TDS usam `net.DialTimeout` para criar conex√µes TCP diretas ao backend.

### Raz√£o
`*sql.DB` √© uma abstra√ß√£o de alto n√≠vel que gerencia seu pr√≥prio pooling interno,
encapsula a conex√£o TDS, e n√£o exp√µe o `net.Conn` subjacente. O proxy precisa
de acesso raw ao stream TCP para relay transparente. S√£o dois n√≠veis de abstra√ß√£o
incompat√≠veis.

### Para que o pool *sql.DB serve
1. **Health checks** ‚Äî `PingContext(ctx)` / `SELECT 1` nas idle connections
2. **sp_reset_connection** ‚Äî limpeza de estado ao devolver conex√£o ao pool
3. **Warm connections** ‚Äî manter `min_idle` conex√µes pr√©-abertas

### Consequ√™ncias
- ‚ùå O `MaxConnections` do BucketPool controla conex√µes `*sql.DB`, N√ÉO as sess√µes TDS ativas
- ‚úÖ O `coordinator.Acquire/Release` no handler.go √© quem controla o limite de sess√µes TDS
- ‚ö†Ô∏è Esses s√£o dois pools separados ‚Äî pode confundir na leitura do c√≥digo

---

## ADR-004: Bucket Selecionado no Pre-Login (antes do Login7)

**Fase:** 2 \
**Status:** Aceita (limita√ß√£o conhecida)

### Contexto
O protocolo TDS tem esta sequ√™ncia: Pre-Login ‚Üí TLS ‚Üí Login7. O Router
(ADR-001) n√£o consegue ler o Login7 porque ele est√° dentro do stream TLS.

### Decis√£o
O bucket √© selecionado em `pickBucket()` antes do Login7, usando o **primeiro
bucket** da configura√ß√£o como default. O Router implementado (`router.go`) est√°
pronto mas **n√£o √© ativado**.

### Consequ√™ncias
- ‚ùå Todas as conex√µes v√£o para o bucket-001 (HAProxy distribui entre proxies,
  mas cada proxy sempre conecta ao mesmo backend)
- ‚úÖ Para o POC com 3 buckets id√™nticos (mesmo schema), √© aceit√°vel
- üîÆ Para produ√ß√£o: seria necess√°rio routing por IP do client, header customizado,
  ou SNI (Server Name Indication) no TLS ClientHello

---

## ADR-005: Fallback Mode com Divisor Local

**Fase:** 3 \
**Status:** Aceita (em vigor)

### Contexto
Quando o Redis fica indispon√≠vel, o proxy n√£o pode coordenar limites globais.
Sem fallback, o proxy simplesmente recusaria todas as conex√µes.

### Decis√£o
**Fallback mode**: cada inst√¢ncia opera independentemente com um limite local
calculado como `max_connections / local_limit_divisor`. Com 3 proxies e divisor=3,
cada um permite at√© 16 conex√µes (50/3). No pior caso (todos os proxies em fallback),
o total m√°ximo √© 48 (abaixo do limite de 50).

### Raz√£o do divisor=3
- N√∫mero de inst√¢ncias do proxy no POC = 3
- `50 / 3 ‚âà 16` ‚Üí total m√°ximo em fallback = 48 < 50 ‚úÖ
- Se uma inst√¢ncia morrer, as outras 2 √ó 16 = 32 < 50 ‚úÖ
- Configur√°vel via `fallback.local_limit_divisor` no proxy.yaml

### Reconcilia√ß√£o
Quando o Redis volta:
1. Heartbeat detecta Redis dispon√≠vel ‚Üí chama `ExitFallback()`
2. `ExitFallback()` faz re-ping + re-load scripts + `reconcileCounts()`
3. `reconcileCounts()` sincroniza contadores locais para o Redis via pipeline HSET

### Consequ√™ncias
- ‚úÖ Proxy continua servindo mesmo sem Redis
- ‚ùå Sem coordena√ß√£o cross-instance durante fallback (pode exceder limite por bucket
  se novas inst√¢ncias subirem sem saber do divisor correto)
- ‚ùå `local_limit_divisor` √© est√°tico ‚Äî deve ser ‚â• n√∫mero m√°ximo de inst√¢ncias

---

## ADR-006: Heartbeat com Cleanup de Inst√¢ncias Mortas

**Fase:** 3 \
**Status:** Aceita (em vigor)

### Contexto
Se um proxy morre sem graceful shutdown (OOMKill, crash, rede), seus contadores
ficam "presos" no Redis. Conex√µes que ele tinha ativas nunca s√£o liberadas,
reduzindo permanentemente a capacidade dispon√≠vel.

### Decis√£o
Cada proxy faz heartbeat (`SET key TTL=30s`) a cada 10s. A cada 30s, cada proxy
vivo verifica se os outros proxies ainda t√™m heartbeat. Se n√£o:
1. L√™ os contadores do morto (`HGETALL proxy:instance:{id}:conns`)
2. Subtrai dos contadores globais (`DECRBY proxy:bucket:{id}:count`)
3. Remove o morto (`DEL` keys + `SREM` do set)
4. Corrige contadores negativos (prote√ß√£o contra double-cleanup)

### Alternativas descartadas
- **Redis keyspace notifications** ‚Äî confi√°vel mas requer configura√ß√£o extra no Redis
- **Lease-based (SETNX com TTL por conex√£o)** ‚Äî muitas keys, overhead alto
- **Cleanup centralizado (um proxy eleito)** ‚Äî precisa de leader election

### Consequ√™ncias
- ‚úÖ Recupera√ß√£o autom√°tica em ~30s ap√≥s morte de uma inst√¢ncia
- ‚úÖ Qualquer proxy vivo pode fazer o cleanup (sem single point of failure)
- ‚ùå Possibilidade de double-cleanup se dois proxies detectam ao mesmo tempo
  (mitigado: `DECRBY` √© idempotente se combinado com corre√ß√£o de negativos)
- ‚ùå Window de ~30s onde capacidade fica reduzida antes do cleanup

---

## ADR-007: Reutilizar DistributedQueue da Fase 3 em vez de Criar Novos Arquivos

**Fase:** 4 ‚Äî Fila de Espera e Backpressure \
**Status:** Aceita \
**Data:** 2026-02-18

### Contexto
O plano original da Fase 4 listava dois novos arquivos (`queue.go`, `waiter.go`).
Por√©m, a Fase 3 j√° entregou `DistributedQueue` (integra Semaphore + Coordinator)
e `Semaphore.Wait()` (Pub/Sub + polling com timeout). Esses componentes j√°
implementavam ~70% do escopo da Fase 4.

### Decis√£o
**Evoluir a `DistributedQueue` existente** em vez de criar novos arquivos:
1. Adicionar `maxQueueSize` + circuit breaker (rejei√ß√£o imediata quando fila cheia)
2. Adicionar `QueueError` tipado com `IsQueueFull()` / `IsQueueTimeout()`
3. Adicionar `ErrQueueTimeout` (50004) e `ErrQueueFull` (50005) em `tds/error.go`
4. Integrar `DistributedQueue` no `handler.go` (substituindo `coordinator.Acquire` direto)
5. Passar `DistributedQueue` via `main.go ‚Üí Server ‚Üí Session`

### Alternativas descartadas
- **Criar `queue.go` e `waiter.go` novos** ‚Äî duplicaria l√≥gica que j√° existe
  no Semaphore e DistributedQueue
- **Usar BucketPool.waitQueue para TDS** ‚Äî BucketPool gerencia `*sql.DB`,
  n√£o sess√µes TDS (ver ADR-003)

### Consequ√™ncias
- ‚úÖ Zero duplica√ß√£o de c√≥digo ‚Äî evoluiu o que j√° existia
- ‚úÖ M√©tricas `QueueLength` e `QueueWaitDuration` j√° estavam instrumentadas no Semaphore
- ‚úÖ Circuit breaker previne ac√∫mulo ilimitado de goroutines esperando
- ‚úÖ Erros tipados permitem enviar TDS Error espec√≠fico ao client (50004 vs 50005)
- ‚ùå `maxQueueSize` √© por inst√¢ncia, n√£o global ‚Äî em 3 proxies, o total m√°ximo
  na fila pode ser 3 √ó `max_queue_size`

---

## Template para Pr√≥ximas Decis√µes

```markdown
## ADR-NNN: [T√≠tulo da decis√£o]

**Fase:** N ‚Äî [Nome da fase] \
**Status:** Proposta | Aceita | Substitu√≠da por ADR-XXX \
**Data:** YYYY-MM-DD

### Contexto
[O que motivou esta decis√£o? Qual problema precisava ser resolvido?]

### Decis√£o
[O que foi decidido. Seja espec√≠fico.]

### Alternativas descartadas
[Lista com breve raz√£o de cada descarte]

### Consequ√™ncias
- ‚úÖ [benef√≠cio]
- ‚ùå [tradeoff/limita√ß√£o]
- ‚ö†Ô∏è [risco ou ponto de aten√ß√£o]
```
