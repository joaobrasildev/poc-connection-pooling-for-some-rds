# PHASE-STATUS.md â€” Estado de ExecuÃ§Ã£o por Fase

> **PropÃ³sito:** o agente lÃª este arquivo primeiro e sabe exatamente
> **onde parou**, **o que estÃ¡ feito**, **o que falta**, e **qual Ã© o prÃ³ximo passo**.
>
> **Regra:** ao concluir uma fase, o agente DEVE atualizar este arquivo
> antes de encerrar a sessÃ£o.
>
> **Ãšltima atualizaÃ§Ã£o:** 2026-02-18 (Fase 4 concluÃ­da)

---

## Resumo RÃ¡pido

| Fase | Nome                              | Status         | ObservaÃ§Ã£o |
|------|-----------------------------------|----------------|------------|
| 0    | Infraestrutura Docker             | âœ… ConcluÃ­da   | 11 containers funcionando |
| 1    | Connection Pool Manager           | âœ… ConcluÃ­da   | BucketPool com LIFO, wait queue, min_idle, eviction |
| 2    | TDS Wire Protocol Proxy           | âœ… ConcluÃ­da   | TCP relay transparente (io.Copy) |
| 3    | CoordenaÃ§Ã£o DistribuÃ­da (Redis)   | âœ… ConcluÃ­da   | Lua scripts, heartbeat, semaphore, fallback |
| 4    | Fila de Espera e Backpressure     | âœ… ConcluÃ­da   | Circuit breaker, erros tipados, integraÃ§Ã£o completa |
| 5    | Observabilidade                   | ğŸ”² NÃ£o iniciada | PrÃ³xima fase |
| 6    | Load Generator e Testes de Carga  | ğŸ”² NÃ£o iniciada | |
| 7    | Hardening e DocumentaÃ§Ã£o          | ğŸ”² NÃ£o iniciada | |

---

## Fase 0 â€” Infraestrutura Docker âœ…

**ConcluÃ­da em:** sessÃ£o de implementaÃ§Ã£o inicial

### O que foi entregue
- `docker-compose.yml` com 11 containers
- 3 SQL Server 2022 (bucket-001/002/003), cada um com `tenant_db`
- Redis 7.4.x standalone
- HAProxy 3.1 (L4 TCP, leastconn, health check HTTP GET :8080)
- Prometheus + Grafana (dashboards nÃ£o customizados ainda)
- Init-db container (seed T-SQL)
- 3 instÃ¢ncias do proxy (proxy-1/2/3)

### Arquivos-chave
| Arquivo | ConteÃºdo |
|---------|----------|
| `deployments/docker-compose.yml` | Todos os 11 services |
| `deployments/haproxy/haproxy.cfg` | L4 TCP balancing entre proxy-1/2/3 |
| `configs/proxy.yaml` | Config do proxy (ports, timeouts, redis, buckets ref) |
| `configs/buckets.yaml` | 3 buckets com host/port/max_connections |
| `prometheus/prometheus.yml` | Scrape config para proxy-1/2/3 |
| `Dockerfile` | Multi-stage: golang:1.24-alpine â†’ alpine:3.19 |

### ValidaÃ§Ã£o feita
- `docker compose up` sobe todos os containers
- `sqlcmd` conecta em cada SQL Server via proxy
- Prometheus scrape funcionando

### DiferenÃ§as vs Plano Original
- 3 buckets em vez de 5 (economia de RAM â€” cada SQL Server usa ~2GB)
- HAProxy 3.1 em vez de 2.9

---

## Fase 1 â€” Connection Pool Manager âœ…

**ConcluÃ­da em:** sessÃ£o de implementaÃ§Ã£o da Fase 1

### O que foi entregue
- `BucketPool` com LIFO idle stack, wait queue com timeout, eviction, min_idle
- Ciclo: Acquire â†’ Use â†’ Release â†’ sp_reset_connection â†’ return to pool
- Health check periÃ³dico (PingContext / SELECT 1)
- Config loader (proxy.yaml + buckets.yaml)

### Arquivos-chave
| Arquivo | LOC | ConteÃºdo |
|---------|-----|----------|
| `internal/pool/pool.go` | 443 | BucketPool (Acquire/Release/evict/refill) |
| `internal/pool/connection.go` | 175 | PooledConn wrapper com state/pin tracking |
| `internal/pool/health.go` | 61 | PingContext health checker |
| `internal/pool/manager.go` | 134 | Manager orquestrando N pools |
| `internal/config/config.go` | ~100 | YAML loader |
| `pkg/bucket/bucket.go` | 50 | Bucket struct |

### âš ï¸ Ponto importante para fases futuras
O `BucketPool` gerencia conexÃµes `*sql.DB` â€” que **nÃ£o sÃ£o usadas para trÃ¡fego
TDS** (ver ADR-003 em DECISIONS.md). O controle de limites para sessÃµes TDS
Ã© feito pelo `coordinator.Acquire/Release` (Fase 3).

---

## Fase 2 â€” TDS Wire Protocol Proxy âœ…

**ConcluÃ­da em:** sessÃ£o de implementaÃ§Ã£o da Fase 2

### O que foi entregue
- TCP listener na porta 1433
- Pre-Login: proxy lÃª, faz forward ao backend, devolve resposta ao client
- ApÃ³s Pre-Login: relay TCP transparente (`io.Copy` bidirecional)
- Login7 parser implementado (extrai database, username, server_name)
- Router por database name implementado (mas **nÃ£o ativado** â€” ver ADR-004)
- Pinning detector implementado (InspectPacket/InspectResponse â€” mas **nÃ£o ativado** â€” ver ADR-001)
- Relay com PacketCallback implementado (mas substituÃ­do por `io.Copy` â€” ver ADR-001)
- TDS Error builder (envia erro TDS ao client em caso de falha)

### Arquivos-chave
| Arquivo | LOC | ConteÃºdo |
|---------|-----|----------|
| `internal/proxy/handler.go` | 293 | Session handler (Pre-Login + io.Copy relay) |
| `internal/proxy/listener.go` | 158 | TCP listener + accept loop |
| `internal/proxy/router.go` | 136 | Router por Login7 database (NÃƒO ATIVADO) |
| `internal/tds/packet.go` | 266 | TDS header/packet parsing |
| `internal/tds/prelogin.go` | 209 | Pre-Login parse + marshal |
| `internal/tds/login7.go` | 173 | Login7 parser |
| `internal/tds/pinning.go` | 374 | Pin detection (NÃƒO ATIVADO) |
| `internal/tds/relay.go` | 173 | Relay com callback (NÃƒO USADO â€” io.Copy em vez) |
| `internal/tds/error.go` | 179 | TDS ERROR token builder |

### âš ï¸ CÃ³digo implementado mas NÃƒO ativado (impacta Fase 4)
1. **`router.go`** â€” estÃ¡ pronto, seria ativado mudando `pickBucket()` em handler.go
2. **`pinning.go`** â€” InspectPacket/InspectResponse completos, precisam de integraÃ§Ã£o
3. **`relay.go`** â€” Relay com PacketCallback existe, mas handler.go usa `io.Copy`

### ValidaÃ§Ã£o feita
- `sqlcmd -S localhost,1433 -U sa -P ... -d tenant_db` funciona via proxy
- SELECT, INSERT, UPDATE, transactions, stored procedures â€” tudo funcional
- ConexÃ£o TLS end-to-end entre client e backend (proxy nÃ£o interfere)

---

## Fase 3 â€” CoordenaÃ§Ã£o DistribuÃ­da (Redis) âœ…

**ConcluÃ­da em:** sessÃ£o de implementaÃ§Ã£o da Fase 3

### O que foi entregue
- `RedisCoordinator` com Acquire/Release atÃ´micos via Lua EVALSHA
- Scripts Lua embeddados via `//go:embed`
- Heartbeat periÃ³dico (SET com TTL=30s, intervalo 10s)
- Cleanup de instÃ¢ncias mortas (detecta heartbeat expirado, corrige contadores)
- SemÃ¡foro distribuÃ­do (Pub/Sub + polling para wait quando bucket estÃ¡ lotado)
- Fallback mode (Redis indisponÃ­vel â†’ limite local com divisor)
- ReconciliaÃ§Ã£o ao sair de fallback
- IntegraÃ§Ã£o no handler.go: Acquire antes de conectar ao backend, Release no cleanup

### Arquivos-chave
| Arquivo | LOC | ConteÃºdo |
|---------|-----|----------|
| `internal/coordinator/redis.go` | 479 | RedisCoordinator (Acquire/Release/Fallback/reconcile) |
| `internal/coordinator/heartbeat.go` | 196 | Heartbeat + cleanupDeadInstances |
| `internal/coordinator/semaphore.go` | 135 | SemÃ¡foro distribuÃ­do (Pub/Sub + poll) |
| `internal/coordinator/lua/acquire.lua` | 33 | Atomic check-and-increment |
| `internal/coordinator/lua/release.lua` | 38 | Atomic decrement + PUBLISH |
| `internal/queue/distributed.go` | 116 | DistributedQueue (Semaphore + Coordinator) |

### ValidaÃ§Ã£o feita
- 3 proxies respeitando limite global de 50 conexÃµes por bucket
- Kill de um proxy â†’ heartbeat detecta â†’ cleanup em ~30s â†’ capacidade recuperada
- Redis down â†’ fallback mode â†’ Redis up â†’ reconciliaÃ§Ã£o automÃ¡tica

---

## Fase 4 â€” Fila de Espera e Backpressure âœ…

**ConcluÃ­da em:** 2026-02-18 
**ADR:** ADR-007 (reutilizar DistributedQueue da Fase 3)

### O que foi entregue
- `DistributedQueue` evoluÃ­da com circuit breaker (`maxQueueSize`)
- `QueueError` tipado com `IsQueueFull()` / `IsQueueTimeout()`
- `ErrQueueTimeout` (50004) e `ErrQueueFull` (50005) em `tds/error.go`
- `handler.go` agora usa `dqueue.Acquire()` em vez de `coordinator.Acquire()` direto
- Config `max_queue_size` (default: 1000) adicionada a `ProxyConfig` e `proxy.yaml`
- Pipeline completo: `main.go` â†’ `NewDistributedQueue()` â†’ `Server` â†’ `Session`

### Arquivos modificados
| Arquivo | MudanÃ§a |
|---------|--------|
| `internal/queue/distributed.go` | +circuit breaker, +QueueError, +IsQueueFull/IsQueueTimeout, assinatura NewDistributedQueue mudou |
| `internal/tds/error.go` | +ErrQueueTimeout (50004), +ErrQueueFull (50005) |
| `internal/proxy/handler.go` | +dqueue field, usa dqueue.Acquire com erros tipados |
| `internal/proxy/listener.go` | +dqueue field, NewServer recebe dqueue |
| `cmd/proxy/main.go` | +cria DistributedQueue, passa ao NewServer |
| `internal/config/config.go` | +MaxQueueSize no ProxyConfig + default |
| `configs/proxy.yaml` | +max_queue_size: 1000 |

### Fluxo de aquisiÃ§Ã£o (atualizado)
```
Client conecta â†’ Pre-Login â†’ pickBucket()
       â†“
dqueue.Acquire(bucketID)
       â†“ fast path ok?
  [sim] â†’ slot adquirido â†’ dial backend â†’ relay
  [nÃ£o] â†’ fila cheia (circuit breaker)?
            [sim] â†’ TDS Error 50005 (ErrQueueFull)
            [nÃ£o] â†’ Semaphore.Wait (Pub/Sub + polling)
                      â†“ timeout?
                  [sim] â†’ TDS Error 50004 (ErrQueueTimeout)
                  [nÃ£o] â†’ slot adquirido â†’ dial backend â†’ relay
```

### MÃ©tricas populadas nesta fase
| MÃ©trica | Labels | Status |
|---------|--------|--------|
| `proxy_queue_length` | `bucket_id` | âœ… Populada (incrementDepth/decrementDepth) |
| `proxy_queue_wait_seconds` | `bucket_id` | âœ… Populada (Semaphore.Wait) |
| `proxy_connections_total` | `bucket_id`, `status` | âœ… Novos status: `acquired`, `acquired_after_wait`, `timeout`, `cancelled`, `rejected_queue_full` |
| `proxy_connection_errors_total` | `bucket_id`, `error_type` | âœ… Novos tipos: `queue_full`, `queue_timeout` |

### ValidaÃ§Ã£o feita

#### 1. Build
- `go build ./...` â€” compila sem erros âœ…

#### 2. Infraestrutura
- `docker compose up -d --build` â€” 11 containers sobem healthy âœ…
- init-db: "All databases initialized and seeded!" âœ…

#### 3. Smoke tests
| Teste | Resultado |
|-------|-----------|
| `SELECT 1` via proxy | âœ… OK |
| `INSERT` + `SELECT` via proxy | âœ… OK (tenant test-phase4, id=101) |
| `BEGIN TRAN` + `INSERT` + `COMMIT` via proxy | âœ… OK (order ORD-PHASE4-001) |
| Stored procedure `sp_connection_info` via HAProxy | âœ… OK |
| 10 conexÃµes concorrentes (script Go) | âœ… 10/10 OK |
| 20 holders + 5 extras (fila de espera) | âœ… 25/25 OK |

#### 4. Queue Timeout (TDS Error 50004)
- **Setup:** `max_connections` reduzido para 3 no Redis, 3 holders saturando o bucket
- **Comportamento:** conexÃ£o extra entra na fila, aguarda 30s, recebe timeout
- **Log confirmado:** `[dqueue] Wait timed out for bucket bucket-001 after 30.006490348s`
- **Log confirmado:** `[session:25] Queue acquire failed for bucket bucket-001: queue timeout`
- **MÃ©trica:** `proxy_connection_errors_total{error_type="queue_timeout"} 1` âœ…
- **MÃ©trica:** `proxy_connections_total{status="timeout"} 1` âœ…
- **Tempo:** ~30s (consistente com `queue_timeout: 30s`) âœ…

#### 5. Circuit Breaker (TDS Error 50005)
- **Setup:** `max_queue_size=2`, `queue_timeout=10s`, `max_connections=3` (Redis)
- **Carga:** 3 holders + 10 conexÃµes extras em paralelo via HAProxy
- **Comportamento:** 4 conexÃµes rejeitadas instantaneamente (~18ms) pelo circuit breaker
- **Log confirmado:** `[dqueue] Circuit breaker: rejecting request for bucket bucket-001 (queue depth=2, max=2)` (4 ocorrÃªncias)
- **MÃ©trica:** `proxy_connection_errors_total{error_type="queue_full"} 4` (total entre 3 proxies) âœ…
- **MÃ©trica:** `proxy_connections_total{status="rejected_queue_full"} 4` (total entre 3 proxies) âœ…

#### 6. MÃ©tricas Prometheus (`/metrics`)
| MÃ©trica | Valor verificado | Status |
|---------|-----------------|--------|
| `proxy_connections_total{status="acquired"}` | 41+ (distribuÃ­do entre 3 proxies) | âœ… |
| `proxy_connections_total{status="acquired_after_wait"}` | 1 | âœ… |
| `proxy_connections_total{status="timeout"}` | 1 | âœ… |
| `proxy_connections_total{status="rejected_queue_full"}` | 4 | âœ… |
| `proxy_queue_length{bucket_id="bucket-001"}` | 0 (correto, fila vazia) | âœ… |
| `proxy_queue_wait_seconds_sum` | 11.009s | âœ… |
| `proxy_connection_errors_total{error_type="queue_timeout"}` | 9+ | âœ… |
| `proxy_connection_errors_total{error_type="queue_full"}` | 4 | âœ… |

#### 7. Health Check
- `GET /health/live` em todos os 3 proxies â†’ `{"status":"alive"}` âœ…

#### 8. Logs
- Nenhum panic, fatal, ou erro inesperado nos logs dos 3 proxies âœ…
- Todos os logs de circuit breaker, timeout e acquired_after_wait confirmados âœ…

---

## Fase 5 â€” Observabilidade ğŸ”²

**Status:** NÃ£o iniciada \
**ReferÃªncia:** `02-PLANO-DE-EXECUCAO.md` seÃ§Ã£o "Fase 5"

### Escopo resumido
- Dashboard Grafana customizado (7 painÃ©is definidos no plano)
- Popular mÃ©tricas que existem mas estÃ£o vazias (ver CONTRACTS.md seÃ§Ã£o 10)
- Alerting rules (queue > threshold, errors > threshold)

### MÃ©tricas registradas mas NÃƒO populadas (a resolver)
- `TDSPacketsTotal` â€” requer relay com parsing (depende de ADR-001)
- `QueryDuration` â€” idem
- `PinningDuration` â€” requer pinning ativo (depende de ADR-001)

---

## Fase 6 â€” Load Generator e Testes de Carga ğŸ”²

**Status:** NÃ£o iniciada \
**ReferÃªncia:** `02-PLANO-DE-EXECUCAO.md` seÃ§Ã£o "Fase 6"

### Escopo resumido
- Load generator em Go usando `go-mssqldb` como driver
- 9 cenÃ¡rios de teste definidos (baseline, burst, steady state, instance failure,
  redis failure, uneven distribution, scale out, long transactions, prepared storm)
- Query mix: 60% simples, 15% prepared, 15% transactions, 5% sprocs, 5% DDL

### DependÃªncias
- Fase 4 (fila precisa funcionar para cenÃ¡rios de burst)
- Fase 5 (mÃ©tricas precisam estar populadas para validar resultados)

---

## Fase 7 â€” Hardening e DocumentaÃ§Ã£o ğŸ”²

**Status:** NÃ£o iniciada \
**ReferÃªncia:** `02-PLANO-DE-EXECUCAO.md` seÃ§Ã£o "Fase 7"

### Escopo resumido
- TLS termination (resolveria ADR-001 se necessÃ¡rio)
- Graceful shutdown (drain connections) â€” âš ï¸ parcialmente implementado em `cmd/proxy/main.go`
- Connection leak detector
- Retry automÃ¡tico em falhas transientes
- Rate limiting por tenant
- README.md completo, diagramas, runbook

---

## Checklist do Agente ao Iniciar uma SessÃ£o

```
1. Ler PHASE-STATUS.md              â†’ saber onde parou
2. Ler ARCHITECTURE.md              â†’ entender topologia e fluxo
3. Ler CONTRACTS.md (se necessÃ¡rio)  â†’ consultar assinaturas sem abrir .go
4. Ler DECISIONS.md (se necessÃ¡rio)  â†’ nÃ£o refazer decisÃµes jÃ¡ tomadas
5. Ler a seÃ§Ã£o da fase no 02-PLANO-DE-EXECUCAO.md â†’ requisitos detalhados
6. Iniciar implementaÃ§Ã£o
7. Ao concluir â†’ ATUALIZAR ESTE ARQUIVO antes de encerrar
```

---

## HistÃ³rico de AtualizaÃ§Ãµes

| Data | Fase | AÃ§Ã£o |
|------|------|------|
| 2026-02-18 | 0-3 | Documento criado com estado retroativo das fases 0-3 |
| 2026-02-18 | 4   | Fase 4 concluÃ­da â€” circuit breaker, erros tipados, integraÃ§Ã£o dqueue (ADR-007) |
| 2026-02-18 | 4   | ValidaÃ§Ã£o E2E completa: smoke tests, queue timeout (50004), circuit breaker (50005), mÃ©tricas, health, logs |
